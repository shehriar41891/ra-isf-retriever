{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1_XoclyveajaiDjERstWJBjrXhqjzRipz",
      "authorship_tag": "ABX9TyPM1k9rKcD7v8DL2ID2aOvD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shehriar41891/ra-isf-retriever/blob/Complete_code/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installation and Setups"
      ],
      "metadata": {
        "id": "YR6ClYrgj8gO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c28veBzBhTh2",
        "outputId": "36c6dec0-5231-408c-88b3-3f0bc5a13575"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into '/content/drive/MyDrive/ra-isf'...\n",
            "remote: Enumerating objects: 287, done.\u001b[K\n",
            "remote: Counting objects: 100% (287/287), done.\u001b[K\n",
            "remote: Compressing objects: 100% (208/208), done.\u001b[K\n",
            "remote: Total 287 (delta 67), reused 283 (delta 66), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (287/287), 3.72 MiB | 5.47 MiB/s, done.\n",
            "Resolving deltas: 100% (67/67), done.\n",
            "Updating files: 100% (160/160), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/shehriar41891/ra-isf-retriever.git /content/drive/MyDrive/ra-isf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TndCx7P2jdNP",
        "outputId": "7678508e-304c-4b69-9dc3-ea462fc6a672"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "config.py\t      main_gpt.py\tREADME.md\t      test.py\n",
            "contriever_config.py  main.py\t\trequirement.txt       utils.py\n",
            "data\t\t      psgs_w100.tsv.gz\tretrieval_contriever  wikipedia_embeddings.tar\n",
            "dataset\t\t      __pycache__\trun.sh\n",
            "evaluation.png\t      ra-isf.png\tsource\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/ra-isf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wpu74jZbkBWd",
        "outputId": "2607c706-4e99-46dc-cd76-86c3e7b01613"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/ra-isf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HF8RHhmCkGqI",
        "outputId": "f72af318-948f-4f75-ff3b-25ab41cbb9de"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "config.py\t      dataset\t      main.py\t   README.md\t\t run.sh   utils.py\n",
            "contriever_config.py  evaluation.png  __pycache__  requirement.txt\t source\n",
            "data\t\t      main_gpt.py     ra-isf.png   retrieval_contriever  test.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install libxml2-dev libxslt1-dev libblas-dev liblapack-dev -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LL71iPORliXq",
        "outputId": "8c9d6910-a0bc-4bd0-821e-d3395b862dfc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "liblapack-dev is already the newest version (3.10.0-2ubuntu1).\n",
            "libxml2-dev is already the newest version (2.9.13+dfsg-1ubuntu0.4).\n",
            "Suggested packages:\n",
            "  liblapack-doc\n",
            "The following NEW packages will be installed:\n",
            "  libblas-dev libxslt1-dev\n",
            "0 upgraded, 2 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 383 kB of archives.\n",
            "After this operation, 3,143 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libblas-dev amd64 3.10.0-2ubuntu1 [164 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libxslt1-dev amd64 1.1.34-4ubuntu0.22.04.1 [219 kB]\n",
            "Fetched 383 kB in 1s (304 kB/s)\n",
            "Selecting previously unselected package libblas-dev:amd64.\n",
            "(Reading database ... 123623 files and directories currently installed.)\n",
            "Preparing to unpack .../libblas-dev_3.10.0-2ubuntu1_amd64.deb ...\n",
            "Unpacking libblas-dev:amd64 (3.10.0-2ubuntu1) ...\n",
            "Selecting previously unselected package libxslt1-dev:amd64.\n",
            "Preparing to unpack .../libxslt1-dev_1.1.34-4ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libxslt1-dev:amd64 (1.1.34-4ubuntu0.22.04.1) ...\n",
            "Setting up libxslt1-dev:amd64 (1.1.34-4ubuntu0.22.04.1) ...\n",
            "Setting up libblas-dev:amd64 (3.10.0-2ubuntu1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install -r requirement.txt -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVzG_fp1kKoF",
        "outputId": "a36a50f7-68bd-42df-9f69-fc77eba87579"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/86.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==2.0.1+cu118 (from versions: 1.11.0, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 2.0.0, 2.0.1, 2.1.0, 2.1.1, 2.1.2, 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.4.0, 2.4.1, 2.5.0, 2.5.1)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==2.0.1+cu118\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install openai -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IT8unzVl-kX",
        "outputId": "59477a52-6021-477a-9ed5-8724432d8c18"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/386.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.9/386.9 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/76.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/78.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/325.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.2/325.2 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/58.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install faiss-cpu -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dl6cECm8mIAX",
        "outputId": "e09c90ae-3bf8-4f2a-9914-c395c817d1fa"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/27.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/27.5 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:04\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/27.5 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/27.5 MB\u001b[0m \u001b[31m181.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m18.8/27.5 MB\u001b[0m \u001b[31m215.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m26.8/27.5 MB\u001b[0m \u001b[31m227.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m220.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m220.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Downloading Preprocessed Passage"
      ],
      "metadata": {
        "id": "xAGhzgQfnPBQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p data/retrieval_wiki && cd data/retrieval_wiki"
      ],
      "metadata": {
        "id": "oBkY32TvnUx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://dl.fbaipublicfiles.com/dpr/wikipedia_split/psgs_w100.tsv.gz\n",
        "!wget https://dl.fbaipublicfiles.com/contriever/embeddings/contriever-msmarco/wikipedia_embeddings.tar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LcSOUVnnlhH",
        "outputId": "10be0171-8348-437a-e0fe-d6103c1a0fbd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-10-30 12:18:16--  https://dl.fbaipublicfiles.com/dpr/wikipedia_split/psgs_w100.tsv.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.163.189.14, 3.163.189.108, 3.163.189.96, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.163.189.14|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4694541059 (4.4G) [application/gzip]\n",
            "Saving to: ‘psgs_w100.tsv.gz’\n",
            "\n",
            "psgs_w100.tsv.gz      7%[>                   ] 315.55M  65.9MB/s    eta 65s    ^C\n",
            "--2024-10-30 12:18:22--  https://dl.fbaipublicfiles.com/contriever/embeddings/contriever-msmarco/wikipedia_embeddings.tar\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.163.189.14, 3.163.189.108, 3.163.189.96, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.163.189.14|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 32499691520 (30G) [application/x-tar]\n",
            "Saving to: ‘wikipedia_embeddings.tar’\n",
            "\n",
            "wikipedia_embedding 100%[===================>]  30.27G  42.7MB/s    in 12m 51s \n",
            "\n",
            "2024-10-30 12:31:13 (40.2 MB/s) - ‘wikipedia_embeddings.tar’ saved [32499691520/32499691520]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/drive/MyDrive/ra-isf/contriever_config.py\n",
        "import argparse\n",
        "\n",
        "def parse_retriever_arguments():\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--data\",\n",
        "        # required=True,\n",
        "        type=str,\n",
        "        default=None,\n",
        "        help=\".json file containing question and answers, similar format to reader data\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--passages\",\n",
        "        type=str,\n",
        "        default=\"/content/drive/MyDrive/ra-isf/data/retrieval_wiki/psgs_w100.tsv.gz\",\n",
        "        help=\"Path to passages (.tsv file)\"\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--passages_embeddings\",\n",
        "        type=str,\n",
        "        default=\"/content/drive/MyDrive/ra-isf/data/retrieval_wiki/wikipedia_embeddings/*.tar\",\n",
        "        help=\"Glob path to encoded passages\"\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--output_dir\", type=str, default=None, help=\"Results are written to outputdir with data suffix\"\n",
        "    )\n",
        "    parser.add_argument(\"--n_docs\", type=int, default=100, help=\"Number of documents to retrieve per questions\")\n",
        "    parser.add_argument(\n",
        "        \"--validation_workers\", type=int, default=32, help=\"Number of parallel processes to validate results\"\n",
        "    )\n",
        "    parser.add_argument(\"--per_gpu_batch_size\", type=int, default=64, help=\"Batch size for question encoding\")\n",
        "    parser.add_argument(\n",
        "        \"--save_or_load_index\", action=\"store_true\", help=\"If enabled, save index and load index if it exists\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--model_name_or_path\", type=str, default=\"/content/drive/MyDrive/ra-isf/contriever_model\", help=\"path to directory containing model weights and config file\"\n",
        "    )\n",
        "    parser.add_argument(\"--no_fp16\", action=\"store_true\", help=\"inference in fp32\")\n",
        "    parser.add_argument(\"--question_maxlength\", type=int, default=512, help=\"Maximum number of tokens in a question\")\n",
        "    parser.add_argument(\n",
        "        \"--indexing_batch_size\", type=int, default=1000000, help=\"Batch size of the number of passages indexed\"\n",
        "    )\n",
        "    parser.add_argument(\"--projection_size\", type=int, default=768)\n",
        "    parser.add_argument(\n",
        "        \"--n_subquantizers\",\n",
        "        type=int,\n",
        "        default=0,\n",
        "        help=\"Number of subquantizer used for vector quantization, if 0 flat index is used\",\n",
        "    )\n",
        "    parser.add_argument(\"--n_bits\", type=int, default=8, help=\"Number of bits per subquantizer\")\n",
        "    parser.add_argument(\"--lang\", nargs=\"+\")\n",
        "    parser.add_argument(\"--dataset\", type=str, default=\"none\")\n",
        "    parser.add_argument(\"--lowercase\", action=\"store_true\", help=\"lowercase text before encoding\")\n",
        "    parser.add_argument(\"--normalize_text\", action=\"store_true\", help=\"normalize text\")\n",
        "\n",
        "    parsed_args = parser.parse_args()\n",
        "    return parsed_args\n",
        "\n",
        "c_args = parse_retriever_arguments()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUQgEpwcvZSp",
        "outputId": "8dbf63dc-d83e-4339-b71a-19bf678517e6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/drive/MyDrive/ra-isf/contriever_config.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run gpt_main.py"
      ],
      "metadata": {
        "id": "oXpEHA6xlt5X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_AxDjsslz_t",
        "outputId": "cce1889e-c3a3-413c-9a2d-d274ca261419"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "config.py\t      evaluation.png\t__pycache__\t retrieval_contriever  utils.py\n",
            "contriever_config.py  main_gpt.py\tra-isf.png\t run.sh\n",
            "data\t\t      main.py\t\tREADME.md\t source\n",
            "dataset\t\t      psgs_w100.tsv.gz\trequirement.txt  test.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tarfile\n",
        "import os\n",
        "import random\n",
        "\n",
        "# Specify the path to the .tar file\n",
        "tar_path = \"/content/drive/MyDrive/ra-isf/data/retrieval_wiki/wikipedia_embeddings.tar\"\n",
        "\n",
        "# Create the extraction directory if it does not exist\n",
        "extraction_dir = \"/content/drive/MyDrive/ra-isf/data/retrieval_wiki/extracted_files/\"\n",
        "os.makedirs(extraction_dir, exist_ok=True)\n",
        "\n",
        "# Check if the .tar file exists\n",
        "if os.path.isfile(tar_path):\n",
        "    with tarfile.open(tar_path, 'r') as tar:\n",
        "        # Get a list of all passage files\n",
        "        tar_files = tar.getnames()\n",
        "\n",
        "        # Filter to only include passage files (assuming they start with 'wikipedia_embeddings/')\n",
        "        passage_files = [file for file in tar_files if file.startswith(\"wikipedia_embeddings/passages_\")]\n",
        "\n",
        "        # Select a random passage file\n",
        "        random_file = random.choice(passage_files)\n",
        "\n",
        "        # Extract the selected random passage file\n",
        "        tar.extract(random_file, path=extraction_dir)\n",
        "        print(f\"Extracted random file: {random_file} to {extraction_dir}\")\n",
        "else:\n",
        "    print(f\"The specified path is not a file: {tar_path}\")"
      ],
      "metadata": {
        "id": "roeVNKHDrthO"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Specify the extraction directory\n",
        "extraction_dir = \"/content/drive/MyDrive/ra-isf/data/retrieval_wiki/extracted_files/\"\n",
        "\n",
        "# Check if the extraction directory exists\n",
        "if os.path.exists(extraction_dir):\n",
        "    # List all files in the directory\n",
        "    all_files = os.listdir(extraction_dir)\n",
        "\n",
        "    # Create a set to hold unique file types\n",
        "    file_types = set()\n",
        "\n",
        "    print(\"Files in the extraction directory:\")\n",
        "    for file in all_files:\n",
        "        # Get the file extension\n",
        "        file_extension = os.path.splitext(file)[1]\n",
        "        file_types.add(file_extension)  # Add the file extension to the set\n",
        "        print(file)  # Print the file name\n",
        "\n",
        "    # Print unique file types\n",
        "    print(\"\\nUnique file types:\")\n",
        "    for file_type in file_types:\n",
        "        print(file_type if file_type else \"No extension\")\n",
        "else:\n",
        "    print(f\"The specified directory does not exist: {extraction_dir}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUKjMIx53Qm7",
        "outputId": "e3116e46-f988-46de-8a8e-c2400db84377"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files in the extraction directory:\n",
            "wikipedia_embeddings\n",
            "\n",
            "Unique file types:\n",
            "No extension\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gzip\n",
        "import csv\n",
        "\n",
        "# Path to the .gz file\n",
        "file_path = '/content/drive/MyDrive/ra-isf/data/retrieval_wiki/psgs_w100.tsv.gz'\n",
        "\n",
        "# Read and print the first few lines of the file\n",
        "with gzip.open(file_path, 'rt', encoding='utf-8') as fin:\n",
        "    reader = csv.reader(fin, delimiter='\\t')\n",
        "    for i, row in enumerate(reader):\n",
        "        print(row)  # Print the entire row\n",
        "        if i == 4:  # Change this number to read more or fewer lines\n",
        "            break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fGU9Y26MBJp",
        "outputId": "c16c59a8-8963-4666-eef8-9b5bb7975a4c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['id', 'text', 'title']\n",
            "['1', 'Aaron Aaron ( or ; \"Ahärôn\") is a prophet, high priest, and the brother of Moses in the Abrahamic religions. Knowledge of Aaron, along with his brother Moses, comes exclusively from religious texts, such as the Bible and Quran. The Hebrew Bible relates that, unlike Moses, who grew up in the Egyptian royal court, Aaron and his elder sister Miriam remained with their kinsmen in the eastern border-land of Egypt (Goshen). When Moses first confronted the Egyptian king about the Israelites, Aaron served as his brother\\'s spokesman (\"prophet\") to the Pharaoh. Part of the Law (Torah) that Moses received from', 'Aaron']\n",
            "['2', 'God at Sinai granted Aaron the priesthood for himself and his male descendants, and he became the first High Priest of the Israelites. Aaron died before the Israelites crossed the North Jordan river and he was buried on Mount Hor (Numbers 33:39; Deuteronomy 10:6 says he died and was buried at Moserah). Aaron is also mentioned in the New Testament of the Bible. According to the Book of Exodus, Aaron first functioned as Moses\\' assistant. Because Moses complained that he could not speak well, God appointed Aaron as Moses\\' \"prophet\" (Exodus 4:10-17; 7:1). At the command of Moses, he let', 'Aaron']\n",
            "['3', 'his rod turn into a snake. Then he stretched out his rod in order to bring on the first three plagues. After that, Moses tended to act and speak for himself. During the journey in the wilderness, Aaron was not always prominent or active. At the battle with Amalek, he was chosen with Hur to support the hand of Moses that held the \"rod of God\". When the revelation was given to Moses at biblical Mount Sinai, he headed the elders of Israel who accompanied Moses on the way to the summit. While Joshua went with Moses to the top,', 'Aaron']\n",
            "['4', \"however, Aaron and Hur remained below to look after the people. From here on in Exodus, Leviticus and Numbers, Joshua appears in the role of Moses' assistant while Aaron functions instead as the first high priest. The books of Exodus, Leviticus and Numbers maintain that Aaron received from God a monopoly over the priesthood for himself and his male descendants (Exodus 28:1). The family of Aaron had the exclusive right and responsibility to make offerings on the altar to Yahweh. The rest of his tribe, the Levites, were given subordinate responsibilities within the sanctuary (Numbers 3). Moses anointed and consecrated\", 'Aaron']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gzip\n",
        "\n",
        "# Define the passage you want to save\n",
        "passage = ['1', 'Aaron Aaron ( or ; \"Ahärôn\") is a prophet, high priest, and the brother of Moses in the Abrahamic religions. Knowledge of Aaron, along with his brother Moses, comes exclusively from religious texts, such as the Bible and Quran. The Hebrew Bible relates that, unlike Moses, who grew up in the Egyptian royal court, Aaron and his elder sister Miriam remained with their kinsmen in the eastern border-land of Egypt (Goshen). When Moses first confronted the Egyptian king about the Israelites, Aaron served as his brother\\'s spokesman (\"prophet\") to the Pharaoh. Part of the Law (Torah) that Moses received from', 'Aaron']\n",
        "\n",
        "# Create a TSV file with the passage\n",
        "tsv_file_path = '/content/drive/MyDrive/ra-isf/data/retrieval_wiki/passage.tsv'\n",
        "with open(tsv_file_path, 'w', encoding='utf-8') as f:\n",
        "    f.write('\\t'.join(passage) + '\\n')\n",
        "\n",
        "# Compress the TSV file into a .gz format\n",
        "gz_file_path = '/content/drive/MyDrive/ra-isf/data/retrieval_wiki/passage.tsv.gz'\n",
        "with gzip.open(gz_file_path, 'wt', encoding='utf-8') as gz_file:\n",
        "    with open(tsv_file_path, 'rt', encoding='utf-8') as tsv_file:\n",
        "        gz_file.writelines(tsv_file.readlines())\n",
        "\n",
        "# Optionally, remove the TSV file after creating the .gz file\n",
        "os.remove(tsv_file_path)\n",
        "\n",
        "print(f\"Passage saved to {gz_file_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RcazP-lqgYUy",
        "outputId": "34aefedd-682a-4282-d7e2-bd632dabcdf0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Passage saved to /content/drive/MyDrive/ra-isf/data/retrieval_wiki/passage.tsv.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python main_gpt.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDd12xTNMQAs",
        "outputId": "b9f8df12-c4bb-4a7a-a37e-73bffa17b20e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------experiment args---------------\n",
            "iteration_max_time:3\n",
            "temperature:0\n",
            "max_length:256\n",
            "type_list_file:./src/format/entity_type_list.txt\n",
            "prompt_id:324\n",
            "infer_num:5\n",
            "engine:llama2-13b\n",
            "api_key:\n",
            "base_model_path:gpt2\n",
            "self_knowledge_model_path:gpt2\n",
            "passage_relevance_model_path:gpt2\n",
            "task_decomposition_model_path:gpt2\n",
            "data_path:dataset/natural_question/nq_open.json\n",
            "output_path:/root/workspace/ra-isf/output/output.json\n",
            "test_start:0\n",
            "test_end:full\n",
            "\n",
            "---------------------------------------------\n",
            "---------------experiment args---------------\n",
            "data:None\n",
            "passages:/content/drive/MyDrive/ra-isf/data/retrieval_wiki/psgs_w100.tsv.gz\n",
            "passages_embeddings:/content/drive/MyDrive/ra-isf/data/retrieval_wiki/wikipedia_embeddings/*.tar\n",
            "output_dir:None\n",
            "n_docs:100\n",
            "validation_workers:32\n",
            "per_gpu_batch_size:64\n",
            "save_or_load_index:False\n",
            "model_name_or_path:/content/drive/MyDrive/ra-isf/contriever_model\n",
            "no_fp16:False\n",
            "question_maxlength:512\n",
            "indexing_batch_size:1000000\n",
            "projection_size:768\n",
            "n_subquantizers:0\n",
            "n_bits:8\n",
            "lang:None\n",
            "dataset:none\n",
            "lowercase:False\n",
            "normalize_text:False\n",
            "\n",
            "---------------------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Loading model from: facebook/contriever-msmarco\n",
            "Indexing passages from files ['/content/drive/MyDrive/ra-isf/data/retrieval_wiki/extracted_files/wikipedia_embeddings/passages_03']\n",
            "Loading file /content/drive/MyDrive/ra-isf/data/retrieval_wiki/extracted_files/wikipedia_embeddings/passages_03\n",
            "Total data indexed 1000000\n",
            "Total data indexed 1313457\n",
            "Data indexing completed.\n",
            "Indexing time: 40.1 s.\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMWOIJzZNcWw",
        "outputId": "a5f55204-2a25-4974-9d41-8e8f0fbff6fc"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "config.py\t      evaluation.png\t__pycache__\t retrieval_contriever  utils.py\n",
            "contriever_config.py  main_gpt.py\tra-isf.png\t run.sh\n",
            "data\t\t      main.py\t\tREADME.md\t source\n",
            "dataset\t\t      psgs_w100.tsv.gz\trequirement.txt  test.py\n"
          ]
        }
      ]
    }
  ]
}